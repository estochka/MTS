{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark==3.3.1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport scipy.sparse as sparse\n\nimport pyspark.sql.functions as F\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import IntegerType, StringType\n\n# ----------------------\nimport bisect\nimport glob","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# not ness\ndef age_bucket(x):\n    return bisect.bisect_left([-1,25,35,45,55,65], x)\n#test and train url\nurl_both = pd.read_csv('/kaggle/input/test-mts/train_test.csv')['0'].to_list()\n#tar id\ntarget_id = pd.read_parquet('/kaggle/input/test-mts/targets.pqt')['user_id'].to_list()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spark = SparkSession.builder \\\n    .master('local') \\\n    .appName(\"MTS\") \\\n    .getOrCreate()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = spark.read.parquet('/kaggle/input/test-mts/MTS/*.par*')\ndf.printSchema()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"____________________\n## URL part","metadata":{}},{"cell_type":"code","source":"# pivot urls top per user\ndef top_urls(path: str, num: int = 11) -> None:\n    urls = df.select('user_id','url_host','request_cnt')\n    urls = urls.groupby('user_id', 'url_host').agg(F.sum('request_cnt').alias('rsum'))\n    win = Window().partitionBy('user_id').orderBy(F.col('rsum').desc())\n    urls = urls.withColumn('row_num',  F.row_number().over(win)).where(F.col('row_num') < (num + 1))\n    urls.groupBy('user_id').pivot('row_num').agg(F.first('url_host')).write.csv(path, header='true')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def url_more_one_req(path: str) -> None:\n    win = Window().partitionBy('user_id').orderBy(F.col('sum(request_cnt)').desc())\n    df.select('user_id','url_host','request_cnt').filter(F.col('request_cnt') > 1) \\\n                                                 .groupby('user_id','url_host').agg(F.sum('request_cnt')) \\\n                                                 .withColumn('row_num', F.row_number().over(win)) \\\n                                                 .write.csv(path, header='true')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# social network part\ndef social_network(path: str) -> None:\n    sn_list = ['m.zen.yandex.ru', 'dzen.ru', 'zen.yandex.ru', \n               'm.vk.com','vk.com',\n               'm.ok.ru', 'm.odnoklassniki.ru', 'ok.ru',\n               'm.facebook.com', 'facebook.com', 'l.facebook.com',\n               'instagram.com', 'l.instagram.com', \n               't.co', 'twitter.com',\n               'youtube.com',\n               'linkedin.com']\n    df = df.select('user_id', 'url_host', 'request_cnt')\n    df = df.filter(F.col('url_host').isin(sn_list))\n    df.groupby('user_id', 'url_host').sum('request_cnt').write.csv(path, header='true')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def list_of_url(path: str) -> None:\n    df = df.select('user_id', 'url_host', 'request_cnt')\n    df = df.groupby('user_id', 'url_host').sum('request_cnt')\n    #win = Window().partitionBy('user_id').orderBy(F.col('sum(request_cnt)').desc())\n    #df = df.withColumn('row_num', F.row_number().over(win))\n    #df.write.csv(path, header='true')\n    df.groupBy('user_id').agg(F.concat_ws(' ', F.collect_set('url_host')).write.csv(path, header='true')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# old. clear URLS\ndef url_prepare(df_temp):\n    #all_url = pd.read_csv('/kaggle/input/test-mts/list_best_uld_2.csv')['0'].to_list()\n\n    #df_temp.groupBy('user_id', 'url_host').agg(F.sum('request_cnt').alias('part_sum')) \\\n    #       .withColumn('all_sum', F.sum('part_sum').over(Window.partitionBy(\"user_id\"))) \\\n    #       .write.csv('urlss', header='true')\n\n    pattern = '(sun\\d+\\-.*?\\.)'\n    df_temp = df_temp.withColumn('url_host', F.when(F.col('url_host').like('%.turbopages%'), \n                                          F.regexp_replace('url_host', '--', '‰')).otherwise(F.col('url_host'))) \\\n                     .withColumn('url_host', F.when(F.col('url_host').like('%.turbopages%'), \n                                          F.regexp_replace('url_host', '-', '.')).otherwise(F.col('url_host'))) \\\n                     .withColumn('url_host', F.when(F.col('url_host').like('%.turbopages%'), \n                                          F.regexp_replace('url_host', '‰', '-')).otherwise(F.col('url_host'))) \\\n                     .withColumn('url_host',  F.regexp_replace('url_host', r'.turbopages', ''))                 \\\n                     .withColumn('url_host', F.regexp_replace('url_host', pattern, ''))\n    # part1 mini-clear\n    return df_temp\n\n\n#    df_temp.groupBy('user_id').agg(F.concat_ws(\" \", F.collect_set(\"url_host\")).alias(\"urles\")).write.csv('urles_v9', header='true')\n\n#    pattern = '^(\\w{1}\\.)|(sun.*?\\.)|(www\\.)'\n#    pattern = '(sun\\d+\\-.*?\\.)'\n#    pattern_2 = '\\..\\.'\n#    df_temp = df_temp.withColumn('url_host', F.regexp_replace('url_host', pattern, ''))\n#    df_temp = df_temp.withColumn('url_host', F.regexp_replace('url_host', pattern_2, '.'))\n\n#     save url to text\n#    df_temp.groupBy('user_id').agg(F.concat_ws(\" \", F.collect_set(\"url_host\")).alias(\"urles\")).write.csv('urles_v0', header='true')\n#    df_temp.groupBy('user_id').agg(F.concat_ws(\" \", F.collect_list(\"url_host\")).alias(\"urles\")).write.csv('urles_v0list', header='true')\n\n#     group_one = '.(\\w*)$'\n#     df_ur = df_temp.withColumn('loc_url', F.regexp_extract('url_host', group_one, 1))\n\n#     df_temp = df_temp.withColumn('url_host', F.regexp_replace('url_host', '.\\w*$', ''))\n\n#     pattern_miniurl = '([\\w_-]*\\.?[\\w_-]+$)'\n#     df_temp = df_temp.withColumn('url_host', F.regexp_extract('url_host', pattern_miniurl, 1))\n\n\n#     df_temp = df_temp.withColumn('url_host', F.when(F.col('url_host').like('%livejournal%'), \n#                                           F.lit('livejournal')).otherwise(F.col('url_host')))\n    \n    \n#     df_temp.groupBy('user_id').agg(F.concat_ws(\" \", F.collect_set(\"url_host\")).alias(\"urles\")).write.csv('urles_v1', header='true')\n#     df_temp.groupBy('user_id').agg(F.concat_ws(\" \", F.collect_list(\"url_host\")).alias(\"urles\")).write.csv('urles_v1list', header='true')\n\n#     df_temp = df_temp.withColumn('url_host', F.when(~F.col('url_host').isin(all_url), \\\n#                                          F.lit('Other')).otherwise(F.col('url_host')))\n#     # save url to text\n   \n#     df_temp.groupBy('user_id').agg(F.concat_ws(\" \", F.collect_set(\"url_host\")).alias(\"urles\")).write.csv('urles_v2', header='true')\n#     df_temp.groupBy('user_id').agg(F.concat_ws(\" \", F.collect_list(\"url_host\")).alias(\"urles\")).write.csv('urles_v2list', header='true')\n\n#     df_temp.groupBy('user_id', 'url_host').agg(F.sum('request_cnt').alias('sums_url')).write.csv('optuna', header='true')\n#     df_temp.write.parquet('before_group.parquet')\n    \n#     df.groupBy('user_id').agg(F.count_distinct('url_host').alias('uniq_url'), \\\n#                           F.count('url_host').alias('all_url')) \\\n#                     .withColumn('url_diff', F.col('uniq_url') / F.col('all_url')).write.csv('url_count', header='true')\n    \n#     df.groupBy('user_id').agg(F.sum('request_cnt').alias('sums'), \n#                               F.count_distinct('date').alias('num_days'), \n#                               (F.datediff(F.max('date'), F.min('date')) + 1).alias('delta_date')).alias('d')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"____________________________\n## GEO part","metadata":{}},{"cell_type":"code","source":"# 2 top region per user \ndef top_reg(path: str):\n    regions = df.select('user_id', 'region_name', 'request_cnt')\n    win = Window().partitionBy('user_id').orderBy(F.col('rsum').desc())\n    regions = regions.groupby('user_id', 'region_name').agg(F.sum('request_cnt').alias('rsum')) \\\n                     .withColumn('row_num',  F.row_number().over(win)).where(F.col('row_num') < 3)\n    regions =  regions.groupBy('user_id').pivot('row_num').agg(F.first('region_name')).toPandas()\n    regions.columns = ['user_id', 'first_reg','sec_reg']\n    regions['sec_reg'] = df2['sec_reg'].fillna(df2['first_reg'])\n    regions.to_parquet(f'{path}.pqt')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all unique region per user. text \ndef region_list(path: str) -> None:\n    df.select('user_id', 'region_name').groupBy('user_id')  \\\n                                       .agg(F.concat_ws(\" \", F.collect_set(\"region_name\")).alias(\"regions\")) \\\n                                       .write.csv(path, header='true')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# seq city per user, weekdays and weekends\n# len of seq\n\n# clear seq\ndef cit(data):\n    city_list = data.split('_')\n    res = [city_list[0]]\n    for itm in city_list[1:]:\n        if res[-1] != itm:\n            res.append(itm)\n    return '_'.join(res)\n\n# create data\ndef mirgation_city(path: str, is_pivot: bool = True) -> None:\n    df = df.select('user_id','date', 'city_name')\n    win = Window().partitionBy('user_id').orderBy(F.col('date').desc())\n    df = df.groupBy('user_id','date', 'city_name').count()\n    df = df.withColumn('row_num',  F.row_number().over(win))\n    df = df.withColumn('dayw', F.dayofweek('date'))\n    df.select('user_id','city_name','row_num', 'dayw').write.csv(path, header='true')\n    \n    if is_pivot:\n        filenames = glob.glob(f'/kaggle/working/{path}/*.csv')\n        res = pd.concat(pd.read_csv(itm) for itm in filenames)\n        \n        res['dayw'] = res['dayw'].isin([1,7]).astype('int')\n        res = res.pivot_table(index='user_id', \n                              columns='dayw', \n                              values='city_name', \n                              aggfunc=lambda x: '_'.join(x))\n        res = res.reset_index()\n        res.columns = ['user_id', 'weekend_list', 'weekdays_list']\n        \n        res['weekend_list'] = res['weekend_list'].map(cit)\n        res['weekdays_list'] = res['weekdays_list'].map(cit)\n        res['wend_cnt_city'] = res['weekend_list'].str.count('_')\n        res['wdays_cnt_city'] = res['weekdays_list'].str.count('_')\n        \n        res['diff'] = res['wend_cnt_city'] - res['wdays_cnt_city'] \n        res.to_parquet(f'{path}.pqt')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__________________________\n## Dates","metadata":{}},{"cell_type":"code","source":"# uniq urls per user. weekend weekdays\ndef weeks(path: str) -> None:\n    # 5 - четверг\n    res = df.select('user_id', 'date', 'url_host').withColumn('date', F.dayofweek('date')) \n    res = res.withColumn('date', F.when(F.col('date').isin([1,7]), 0).otherwise(1))\n    res = res.groupBy('user_id').pivot('date').agg(F.concat_ws(\" \", F.collect_set(\"url_host\")).alias(\"urles\")).toPandas()\n    res.columns = ['user_id', 'wend', 'wdays']\n    res['wend'] = res.apply(lambda x: ' '.join(set(x['wend'].split(' ')) - set(x['wdays'].split(' '))), axis=1)\n    res['wdays'] = res.apply(lambda x: ' '.join(set(x['wdays'].split(' ')) - set(x['wend'].split(' '))), axis=1)\n    res.to_parquet(f'{path}.pqt')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# URLs->word per user. part of day\ndef word_days(path: str, part_day: str, is_explode: bool = True) -> None:\n    # urls\n    # evening, day, morning,  night\n    if part_day:\n        urls = df.filter(F.col('part_of_day') == part_day).select('user_id', 'url_host', 'request_cnt')\n    else:\n        urls = df.select('user_id', 'url_host', 'request_cnt')\n   \n    urls = urls.groupby('user_id', 'url_host').sum('request_cnt')\n\n    pattern = '\\.com\\.|\\.(\\w*)$'\n    urls = urls.withColumn('clear_url', F.regexp_replace('url_host', pattern, ''))\n    urls = urls.withColumn('clear_url', F.regexp_replace('clear_url', '[^A-Za-z]', ' '))\n    urls = urls.withColumn('clear_url', F.regexp_replace('clear_url', '( |^).( |$)', ' '))\n    urls = urls.withColumn('clear_url', F.regexp_replace('clear_url', ' +', ' '))\n\n    if is_explode:\n        urls = urls.withColumn('clear_url', F.explode(F.split('clear_url',' ')))\n        urls = urls.filter(F.col('clear_url') != '')\n        urls = urls.groupby('user_id', 'clear_url').agg(F.sum('rsum').alias('rsum'))\n        \n    \n    win = Window().partitionBy('user_id').orderBy(F.col('rsum').asc())\n    urls = urls.withColumn('row_num', F.row_number().over(win))\n    \n    urls.write.csv(path, header='true')\n\n    # data \"leak\"\n    #urls = urls.join(target, on='user_id', how='inner')\n    #urls = urls.select('clear_url','age','rsum')\n    #urls.groupBy('clear_url').pivot('age').agg(F.sum('rsum'), F.count('rsum')).write.csv('age', header='true')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def last_days_count():\n    df = df.select('user_id', 'date').groupby('user_id', 'date').count()\n    win = Window().partitionBy('user_id').orderBy(F.col('date').desc())\n    df = df.withColumn('row_num', F.row_number().over(win)).where(F.col('row_num') < 6)\n    df.groupby('user_id').pivot('row_num').agg(F.sum('count')).write.csv('last5day', header='true')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__________________________________\n## Other","metadata":{}},{"cell_type":"code","source":"# cpe_manufacturer_name. old\n# data cont only 1 phone for user\ndef phones(path: str) -> None:\n    popular = ['Apple', 'Samsung', 'Huawei', 'Xiaomi']\n    df_temp = df.select('user_id','cpe_manufacturer_name')\n    test = df_temp.withColumn('cpe_manufacturer_name', F.when(~df_temp.cpe_manufacturer_name.isin(popular), \n                                                              F.lit('Other')).otherwise(df_temp.cpe_manufacturer_name))\n    result = test.dropDuplicates(['user_id']).alias('phones')\n    result.write.csv(path, header='true')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mean price for phone. old\n# by cpe_manufacturer_name\ndef mean_price(path: str) -> None:\n    popular = ['Apple', 'Samsung', 'Huawei', 'Xiaomi']\n    df_temp = df.select('price','cpe_manufacturer_name','user_id')\n    df_temp = df_temp.withColumn('cpe_manufacturer_name', F.when(~df_temp.cpe_manufacturer_name.isin(popular), \n                                                                 F.lit('Other')).otherwise(df_temp.cpe_manufacturer_name))\n    win = Window.partitionBy('cpe_manufacturer_name')\n    res = df_temp.withColumn('new_price', F.avg(df_temp.price).over(win))\n    res = res.withColumn('price', F.when(F.isnull(res.price), res.new_price).otherwise(res.price))\n    result = res.groupby('user_id').agg(F.mean('price').alias('price'))\n    result.write.csv(path, header='true')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# OS type. old\ndef ostypes(path: str) -> None:\n    df_temp = df.select('cpe_model_os_type', 'user_id')\n    df_temp = df_temp.withColumn('cpe_model_os_type', F.regexp_replace('cpe_model_os_type', 'Apple iOS', 'iOS'))\n    result = df_temp.groupBy('user_id').agg(F.first('cpe_model_os_type'))\n    result.write.csv(path, header='true')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_______________________________\n## For URL Cluster","metadata":{}},{"cell_type":"code","source":"def modas(col):\n    win  = Window().partitionBy('url_host').orderBy(F.col('sum(request_cnt)').desc())\n    df_moda = df.select(col, 'url_host', 'request_cnt').groupby('url_host', col) \\\n                                .agg(F.sum('request_cnt'))\n    df_moda = df_moda.withColumn('row_num', F.row_number().over(win)).where(F.col('row_num') == 1)\n   # df_moda.write.csv(f'{col}_moda', header='true')\n    df_moda = df_moda.select('url_host', col)\n    return df_moda\n\ndef modas2():\n    reg = modas('region_name')\n    cpe = modas('cpe_manufacturer_name')\n    model = modas('cpe_model_name')\n    oss = modas('cpe_model_os_type')\n    day_part = modas('part_of_day')\n    # group URLs\n    another = df.groupBy('url_host').agg(F.mean('price').alias('price'), \n                           F.count_distinct('user_id').alias('users'), \n                           F.datediff(F.current_date(), F.max('date')).alias('last_date'), \n                           F.mean('request_cnt').alias('mean_req'),\n                           F.sum('request_cnt').alias('sum_req'))\n    urls = another.join(reg, on='url_host', how='left')\n    urls = urls.join(cpe, on='url_host', how='left')\n    urls = urls.join(model, on='url_host', how='left')\n    urls = urls.join(oss, on='url_host', how='left')\n    urls = urls.join(day_part, on='url_host', how='left')\n    urls.write.csv('urls', header='true')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"____________________________\n## Data \"Leak\"","metadata":{}},{"cell_type":"code","source":"# IDs\ndef male_and_female(path: str) -> list:\n    targets = pd.read_csv(path)\n    males = targets[targets['is_male'] == 1]['user_id'].to_list()\n    females = targets[targets['is_male'] == 0]['user_id'].to_list()\n    return [males, females]\n\ndef user_cat(num):\n    targets = pd.read_parquet('/kaggle/input/test-mts/targets.pqt')\n    targets['age'] = targets['age'].map(age_bucket)\n    nums = targets[targets['age']  == num]['user_id'].to_list()\n    return nums","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ages_perc(path: str, age_: int) -> None:\n    df = df.select('user_id', 'url_host', 'request_cnt').filter(F.col('url_host').isin(url_both))\n    df_one = df.filter(F.col('user_id').isin(user_cat(age_))).groupBy('url_host') \\\n                                                             .agg(F.sum('request_cnt').alias('ft_sum'), \\\n                                                                  F.count('request_cnt').alias('ft_count'))\n    df_one.write.csv(path, header='true')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f_m(df):\n    df = df.select('user_id', 'url_host', 'request_cnt')\n    males, females = male_and_female('/kaggle/input/test-mts/is_male_target_414724.csv')\n    df_males = df.filter(F.col('user_id').isin(males)).groupBy('url_host') \\\n                                                      .agg(F.sum('request_cnt').alias('male_sum'),\\\n                                                           F.count('request_cnt').alias('male_count'))\n    df_males.write.csv('males', header='true')\n    df_females = df.filter(F.col('user_id').isin(females)).groupBy('url_host') \\\n                                                          .agg(F.sum('request_cnt').alias('female_sum'),\\\n                                                               F.count('request_cnt').alias('female_count'))\n    df_females.write.csv('females', header='true')\n    df_both = df.groupBy('url_host') \\\n                .agg(F.sum('request_cnt').alias('both_sum'),\\\n                     F.count('request_cnt').alias('both_count'))\n    df_both.write.csv('both', header='true')","metadata":{},"execution_count":null,"outputs":[]}]}